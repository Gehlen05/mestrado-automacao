{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar e ativar uma venv\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4430be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics.functional import accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab20941",
   "metadata": {},
   "source": [
    "### Dataset e AnÃ¡lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TransformaÃ§Ãµes: converte para tensor e normaliza (mÃ©dia=0.5, desvio=0.5)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# ðŸ”¹ Carregar dataset de treino completo (FashionMNIST)\n",
    "full_train_data = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# ðŸ”¹ Dividir em treino (50k) e validaÃ§Ã£o (10k)\n",
    "train_size = 50000\n",
    "val_size = len(full_train_data) - train_size\n",
    "train_data, val_data = random_split(full_train_data, [train_size, val_size])\n",
    "\n",
    "# ðŸ”¹ Carregar dataset de teste\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# ðŸ”¹ DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ee415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Torch size: {test_data[0][0].shape} e labels: {test_data[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes e indÃ­ces\n",
    "print(train_data.dataset.classes)\n",
    "print(train_data.dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c104660",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"image.squeeze() shape: {image.squeeze().shape}\")\n",
    "plt.imshow(image.squeeze())\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.dataset.classes\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ab1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "rows, cols = 5, 5\n",
    "for i in range(1, rows*cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_train_image_batch, first_batch_labels = next(iter(train_loader))\n",
    "first_train_image_batch.shape, first_batch_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274ea3f",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolution layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,\n",
    "        kernel_size=3, stride=1, padding=1)\n",
    "        # Convolution layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,\n",
    "        kernel_size=3, stride=1, padding=1)\n",
    "        # Fully connected layers\n",
    "        # 7x7 is the size of the image after pooling layers\n",
    "        self.fc1 = nn.Linear(7*7*64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        # Layer 1: Convolution -> Activation -> Pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2) # 2x2 max pooling\n",
    "        # Layer 2: Convolution -> Activation -> Pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2) # 2x2 max pooling\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNDropout, self).__init__()\n",
    "        # Convolution layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,\n",
    "        kernel_size=3, stride=1, padding=1)\n",
    "        # Convolution layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,\n",
    "        kernel_size=3, stride=1, padding=1)\n",
    "        # Fully connected layers\n",
    "        # 7x7 is the size of the image after pooling layers\n",
    "        self.fc1 = nn.Linear(7*7*64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1: Convolution -> Activation -> Pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2) # 2x2 max pooling\n",
    "        # Layer 2: Convolution -> Activation -> Pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2) # 2x2 max pooling\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319beb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNDropoutNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNDropoutNorm, self).__init__()\n",
    "        # Convolution layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,\n",
    "        kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # Convolution layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,\n",
    "        kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # Fully connected layers\n",
    "        # 7x7 is the size of the image after pooling layers\n",
    "        self.fc1 = nn.Linear(7*7*64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1: Convolution -> Activation -> Pooling\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNNDropoutNormRes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNDropoutNormRes, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bloco Residual 1\n",
    "        identity = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += F.interpolate(identity, size=x.shape[-2:])  # Se necessÃ¡rio ajustar tamanho\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Segunda convoluÃ§Ã£o + pooling\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Flatten e FC\n",
    "        x = x.view(-1, 7 * 7 * 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab9fe7b",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fa82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda71ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinamento(model, optimizer, num_epochs=5):\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch train [{epoch+1}/{num_epochs}], Loss train: {avg_loss:.4f}\")\n",
    "        \n",
    "        eval_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Backward pass and optimization\n",
    "                eval_loss += loss.item()\n",
    "        avg_loss_eval = eval_loss / len(val_loader)\n",
    "        eval_losses.append(avg_loss_eval)\n",
    "        print(f\"Epoch eval [{epoch+1}/{num_epochs}], Loss eval: {avg_loss_eval:.4f}\")\n",
    "    return model, train_losses, eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DefiniÃ§Ã£o criterion e otimizador para todos modelos\n",
    "def escolha_optimizer(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b6347",
   "metadata": {},
   "source": [
    "### MÃ©tricas e AvaliaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafico_treinamento(nome_modelo, num_epochs, train_losses, eval_losses):\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "    plt.plot(range(1, num_epochs + 1), eval_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{nome_modelo} Train vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{nome_modelo}_loss_curve.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04edeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao(model):\n",
    "    # Prediction loop\n",
    "    predictions = []\n",
    "    model.eval\n",
    "    with torch.inference_mode():  # Disable gradient computation\n",
    "        for X, _ in test_loader:  # `_` means we won't use the labels here\n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # For classification (e.g., binary or multi-class):\n",
    "            predicted_classes = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "            # Append predictions\n",
    "            predictions.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "    print(\"Predictions:\", [class_names[x] for x in predictions])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exatidao_modelo(model):\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_loader:\n",
    "            preds = torch.argmax(model(X), dim=1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y)\n",
    "\n",
    "    # Concatena todos os tensores\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Usa torchmetrics para calcular\n",
    "    acc = accuracy(all_preds, all_labels, task='multiclass', num_classes=10)\n",
    "    # print(f\"ExatidÃ£o: {acc:.4f}\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caae7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao_unica(model, name_model, predictions):\n",
    "    index = torch.randint(len(test_loader), size=[1]).item()\n",
    "    predicted_labels = [class_names[x] for x in predictions]\n",
    "\n",
    "    image, label = test_data[index]\n",
    "    print(f\"Image shape: {image.shape}\")\n",
    "    # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "    # squeeze will get rid of first dimension\n",
    "    # so the shape of image.squeeze() will be [28, 28]\n",
    "    print(f\"image.squeeze() shape: {image.squeeze().shape}\")\n",
    "    plt.imshow(image.squeeze())\n",
    "    plt.title(f\"Real - {class_names[label]} : Predicted - {predicted_labels[index]}\");\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Salvar imagem\n",
    "    plt.savefig(f\"{name_model}_predicao_index{index}.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74155161",
   "metadata": {},
   "source": [
    "### Chamadas treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4221503",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = SimpleCNN()\n",
    "model_cnn_drop = SimpleCNNDropout()\n",
    "model_cnn_norm = SimpleCNNDropoutNorm()\n",
    "model_cnn_res = SimpleCNNDropoutNormRes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cnn = escolha_optimizer(model_cnn)\n",
    "optimizer_drop = escolha_optimizer(model_cnn_drop)\n",
    "optimizer_norm = escolha_optimizer(model_cnn_norm)\n",
    "optimizer_res = escolha_optimizer(model_cnn_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model_cnn, train_losses_cnn, eval_losses_cnn  = treinamento(model_cnn, optimizer_cnn, num_epochs)\n",
    "model_cnn_drop, train_losses_drop, eval_losses_drop  = treinamento(model_cnn_drop, optimizer_drop, num_epochs)\n",
    "model_cnn_norm, train_losses_norm, eval_losses_norm  = treinamento(model_cnn_norm, optimizer_norm, num_epochs)\n",
    "model_cnn_res, train_losses_res, eval_losses_res  = treinamento(model_cnn_res, optimizer_res, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafico_treinamento('cnn', num_epochs, train_losses_cnn, eval_losses_cnn)\n",
    "grafico_treinamento('cnn_drop', num_epochs, train_losses_drop, eval_losses_drop)\n",
    "grafico_treinamento('cnn_norm', num_epochs, train_losses_norm, eval_losses_norm)\n",
    "grafico_treinamento('cnn_res', num_epochs, train_losses_res, eval_losses_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cnn = predicao(model_cnn)\n",
    "predictions_drop = predicao(model_cnn_drop)\n",
    "predictions_norm = predicao(model_cnn_norm)\n",
    "predictions_res = predicao(model_cnn_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc737c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "exatidao_cnn = exatidao_modelo(model_cnn)\n",
    "exatidao_drop = exatidao_modelo(model_cnn_drop)\n",
    "exatidao_norm = exatidao_modelo(model_cnn_norm)\n",
    "exatidao_res = exatidao_modelo(model_cnn_res)\n",
    "print(f\"CNN: {exatidao_cnn}\")\n",
    "print(f\"CNN-DROP: {exatidao_drop}\")\n",
    "print(f\"CNN-NORM: {exatidao_norm}\")\n",
    "print(f\"CNN-RES: {exatidao_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e47cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicao_unica(model_cnn, 'cnn', predictions_cnn)\n",
    "predicao_unica(model_cnn_drop, 'cnn', predictions_drop)\n",
    "predicao_unica(model_cnn_norm, 'cnn', predictions_norm)\n",
    "predicao_unica(model_cnn_res, 'cnn', predictions_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
